% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.2 distribution.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[%
reprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
amsmath,amssymb,
aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-2}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{svg}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}

\begin{document}
	
	\preprint{APS/123-QED}
	
	\title{Manuscript Title:\\with Forced Linebreak}% Force line breaks with \\
	\thanks{A footnote to the article title}%
	
	\author{Ann Author}
	\altaffiliation[Also at ]{Physics Department, XYZ University.}%Lines break automatically or can be forced with \\
	\author{Second Author}%
	\email{Second.Author@institution.edu}
	\affiliation{%
		Authors' institution and/or address\\
		This line break forced with \textbackslash\textbackslash
	}%
	
	\collaboration{MUSO Collaboration}%\noaffiliation
	
	\author{Charlie Author}
	\homepage{http://www.Second.institution.edu/~Charlie.Author}
	\affiliation{
		Second institution and/or address\\
		This line break forced% with \\
	}%
	\affiliation{
		Third institution, the second for Charlie Author
	}%
	\author{Delta Author}
	\affiliation{%
		Authors' institution and/or address\\
		This line break forced with \textbackslash\textbackslash
	}%
	
	\collaboration{CLEO Collaboration}%\noaffiliation
	
	\date{\today}% It is always \today, today,
	%  but any date may be explicitly specified
	
	\begin{abstract}
		Because of its powerful ability to extract fundamental features from complex systems, deep learning has a wide range of applications in areas such as image recognition, speech processing, and biological systems. In particular, in the field of condensed matter physics, deep learning has demonstrated the unique ability to accurately approximate complex physical systems. In the past few years, traditional neural networks have achieved good results in condensed matter physics both in terms of feature extraction and model generation. However, the limitation of configuration size has led to the generation of only single-size configurations, which limits the generalizability of neural networks. Traditional deep learning algorithms assume that data samples are independent of each other, but in real physical configurations, each spin node is associated with other nodes and edges in the configuration. This problem is solved by the emergence of graph neural networks, which are irregular, can adapt to arbitrary size configurations, and can capture the interdependencies between different nodes. Variational self-coding is an unsupervised learning algorithm that can effectively model the distribution of training data. In this paper, we use this feature of graph neural networks in combination with variational self-coding to construct Ising model simulators, and the generated Ising models are very similar to the data simulated by traditional Monte Carlo methods. The results show that graph neural networks are a promising computational tool in the field of condensed matter physics.
	\end{abstract}
	
	%\keywords{Suggested keywords}%Use showkeys class option if keyword
	%display desired
	%\maketitle
	
	%\tableofcontents
	
	\section{introduction}
	
	Over the past year, physicists have started to use deep learning techniques to study the field of condensed matter physics. Most of the tasks are done by supervised learning algorithms. In supervised learning, the algorithm is trained on data with labels, which are assigned to data points. After successful training, high precision predictions can be made for labels of previously unseen data. In addition to supervised learning, there are unsupervised learning algorithms that can find structure in unlabeled data. It has become possible to use unsupervised learning techniques to reproduce the Monte Carlo sampling state of the Ising model, and discovering phase transitions in an unsupervised manner has become mainstream. Restricted Boltzmann machines, variational self-coding, and generative adversarial networks have been widely used for the simulation of Ising models with good results. However, none of these models can take into account the molecule-molecule interactions and the influence of the edges between molecules in the physical model during training. Also, the microstructure of many physical models is irregular, and the generated models are poorly generalized and can generally only accommodate one size at one temperature. This requires a deep neural network that can consider unstructured data and can take into account node-to-node and node-to-edge relationships.

	
	Fortunately, the research on graph neural networks in the field of deep learning has been growing in enthusiasm in recent years, and the excellent ability of graph neural networks to handle unstructured data makes them particularly useful for applications in physics, biology, and chemistry. Most physical models are essentially a graph structure. Compared with traditional convolutional networks, graph convolutional networks are better at encoding structural information of graphs and can learn better and deeper representation features. Graph variational self-coding is an unsupervised graph generation algorithm, which is the product of combining graph neural network and generative network, and it can effectively use the graph structure of data to simulate the distribution of training data.


	
	In this paper, we integrate a graph convolutional network into a graph variational self-encoder framework to construct an Ising model simulator, which is used to simulate Ising model configurations at different temperatures. The trained simulator can effectively generate Ising model states with physical properties that are not different from those obtained from conventional Monte Carlo simulations. It can also be adapted to a wide range of different sizes and significantly reduces the simulation time. We will first introduce the general form of the Ising model, graph variational neural networks and graph convolutional networks. In the core of the paper, we construct a graph variational self-encoder combined with a graph convolutional network to simulate the 2D Ising model at different temperatures and verify the validity of our method by comparing the generated data with those from Monte Carlo simulations.
	
	\section{Ising model and Graph neural networks}
	
	\subsection{Ising model}
	
	In statistical physics, the Ising model is described as the set of binary spins with coupling interactions in some lattice. Considering that N spins s={$s_i$} can take the value $\pm$ 1, where the index i marks the position of the spin $s_i$, the standard Hamiltonian of the Ising system includes only the nearest-neighbor interactions, and each spin direction may be "up" (+1) or "down" (-1), although the generalized model may include long-range interactions and more spin-selective directions. The standard Hamiltonian quantities are:
    \begin{equation}\label{H}
    H = -J\sum_{neighbors}S_i*S_j
	\end{equation}
	\subsection{Graph neural network}
	A graph is a data structure that models a set of objects (nodes) and their relationships (edges). This year, research on analyzing graphs with machine learning methods has received increasing attention due to the powerful expressiveness of graph structures. Graph neural network is a general term for algorithms that use neural networks to learn graph structured data, extract and uncover features and patterns in graph structured data, and meet the needs of graph learning tasks such as clustering, classification, prediction, segmentation, and generation. One motivation for the development of graph neural networks originated from convolutional neural networks. The widespread use of convolutional neural networks has led to breakthroughs in machine learning and opened the era of deep learning. However, convolutional neural networks can only extract potential features in regular Euclidean space data, which cannot correspond well to the complex and variable graph data in reality, and how to apply convolutional neural networks to the non-Euclidean space of graph structure has become the key problem of graph neural network models. The descriptions and definitions of the relevant symbols in graph neural networks are given below
	
	\textbf{Definition 1} \textit{Graph:}The graph is formed by the nodes and the edges connecting the nodes, usually noted as $G=(V,E)$. where $V=\{v_1,v_2,... ,v_n\}$ represents the set of nodes,$E=\{e_1,e_2,... ,e_n\}$ represents the set of edges, and edges can also be represented as $(v_1,v_2)$. Usually nodes are also called vertices or intersections. Edges are also called links or arcs. A generic graph representation is a quintet: $G(V,E,A,X,D)$. Where A represents the adjacency matrix of the graph, X represents the identity matrix of the nodes, and D represents the degree matrix.
	
	\textbf{Definition 2} \textit{Adjacency matrix}: The adjacency matrix of a graph refers to the matrix used to represent the connectivity of the nodes in the graph. This matrix can be binary or weighted. For an undirected graph with N nodes, the adjacency matrix is an N*N real symmetric matrix.
	
	\textbf{Definition 3} \textit{Degree matrix}: The degree of a node represents the number of edges connected to that node. The degree matrix of a graph is the matrix that describes the degree of each node in the graph. The degree matrix is a diagonal matrix, and for undirected graphs, only the incoming degree matrix or the outgoing degree matrix is generally used.
	
	\textbf{Definition 4} \textit{Combinatorial Laplacian matrix}, also known as standard Laplacian matrix, is a combination of diagonal matrix and adjacency matrix:
	\begin{equation}
		L = D - A
	\end{equation}
	The matrix has non-zero elements only at the central node and at the nodes connected to the first order, and zero everywhere else. Laplacian matrix is also a form of graph.
	
	\textbf{Definition 5} \textit{Normalized Laplacian matrix}
	\begin{equation}
		L^{sym} = I - D^{-1/2}AD^{-1/2}
	\end{equation}
	Its element value is :
	\begin{equation}
		L^{sym}_{(i,j)} = 
		\begin{cases}
			1,& \text{i=j,$deg(v_i)\ne0$} \\
			\frac {-1}{\sqrt{deg(v_i)deg(v_j)}},& \text{${i}\ne{j}$ ,$v_i$ and $v_j$ are adjacent}\\
			0,& \text{otherwise}
		\end{cases}
	\end{equation}
	where $deg(v)$ is denoted as the degree of node $v$
	\subsubsection{Graph Convolutional Network}
	A graph convolution network is  derived from the traditional convolutional neural network. Graph convolution can be of two kinds, spectrum-based approach and space-based approach. The spectrum-based approach defines graph convolution from the perspective of graph signal processing by introducing filters, so the spectrum-based graph convolution can be understood as removing noise from the graph signal. The space-based graph convolution method constructs the graph convolution by pooling information from neighboring nodes.
	
	Before understanding graph convolution, we need to have a basic concept of convolution operation. Convolutional networks are first applied to images, where we use the pixel points on the image as features to perform the convolution operation to extract features. The random shared convolution sum is used to obtain a weighted sum of pixel points to extract a particular feature, and then back propagation is used to optimize the convolution kernel parameters to automatically extract features, which is the cornerstone of convolutional neural network to extract features. The convolution operation is equivalent to aggregating the information of surrounding pixels to the central pixel of the convolution kernel and extracting the features of that part.
	
	So we want to use convolution in the graph domain to use a general paradigm for graph feature extraction, similar to image, when we want to get the feature representation of a node, we can aggregate the information of neighboring nodes, which can be understood in this way, each node in the graph is changing its state all the time until the final balance because of the influence of the more distant points of the neighboring node kernel, the closer the neighbor The closer the neighbor, the greater the influence.
	\subsubsection{Graph Convolutional Generalization}	
	For a graph convolutional network, we want to learn its feature representation for each node, and any graph convolution can be written as a nonlinear function such that:
	\begin{equation}\label{key}
		H^{l+1} = f(H^l,A)
	\end{equation}
	$H^0 = X$ is the input to the first layer, $l$ represents the number of layers of the neural network, and A represents the adjacency matrix.

	The traditional graph convolutional network approach is to add information from neighboring nodes to this node:
	\begin{equation}\label{key}
		f(H^l,A) = \sigma(A H^l W^l)
	\end{equation}
	$W^l$ represents the parameter matrix of the lth layer, and $\sigma$ represents the activation function. According to the matrix multiplication it can be seen that each node in the above equation combines the information of the neighboring nodes, but since only the adjacency matrix is used, the diagonal of the adjacency matrix is 0, which cannot reflect the information of the nodes themselves, so we use the Laplacian matrix instead of the adjacency matrix
	\begin{figure}
		\centering
		\includegraphics[scale=0.5]{GNN}
		\caption{This is figure 1}
		\label{fig:gnn}
	\end{figure}
	\begin{equation}\label{key_w}
		f(H^l,A) = \sigma(L H^l W^l)
	\end{equation}
	The Laplace matrix is introduced in the above equation\eqref{key_w}, thus solving the problem of not considering the information self-transfer of its own nodes, but since it is not normalized, we regularize the Laplace matrix to obtain the graph convolution
	\begin{equation}\label{LapalceMatrix}
	f(H^l,A) = \sigma(L^{sym} H^l W^l)	
	\end{equation}
	The above description is in matrix form, we can also look at the formula from the perspective of a single node, for the node feature $h^{l+1}_i$ in the $l+1$ layer, for his neighboring nodes $j \in N$, $N$ is the set of neighboring nodes used by node $i$, so the update formula of the graph neural network can also be depicted as
	\begin{equation}\label{formulaNode}
		h^{l+1}_i = f(h^{l}_i) = \sigma(h^{l}_i \bullet W_1^{l+1} + \sum_j h^{l}_j \bullet W^{l+1}_2)
	\end{equation}
	\subsubsection{Variational auto-encoder}
	A variational self-encoder is a generative model of variational inference, consisting of a network of encoders and decoders, which is a model with hidden variables. In a variational self-encoder, we base on the following assumptions:
	Our sample x is generated by some latent variable z through some mapping, and z is not a fixed value, but obeys a distribution: $z \sim P_{\theta}(z)$, then $x \sim p_{\theta}(x|z)$, where P is a family of distributions determined by the parameter $\theta$, and $ \theta$ is the true distribution that we need to find. So our fundamental goal is to determine the values of $z$ and $\theta$ so that we can obtain the sample x. In order to infer $p_{\theta}(x|z)$, we need to maximize the marginal log likelihood $log(p_{\theta}(x))$ We can rewrite the equation:
	\begin{equation}\label{vae}
		\begin{aligned}
			log(p_{\theta}(x)) &= log \int p_{\theta}(x|z)dz = log \int p_{\theta}(x|z)p(z)dz \\
			&= log \int p_{\theta}(x|z)p(z)\frac{q_{\phi}(z|x)}{q{\phi}(z|x)}dz\\
			&\ge \mathbb E_{z \sim q_{\phi}(z|x)}[log(p_{\theta}(x|z)] - D_{KL}[q_{\phi}(z|x)||p(z)]\\
			&:= \varepsilon(x,\theta,\phi)
		\end{aligned}
	\end{equation}
	We apply Jensen's inequality in the fourth step. Integrating the potential variable z in the second step is usually tricky, so we introduce an approximate posterior distribution $q_{\phi}(z|x)$ with the parameter set $\phi$ and use the variational inference principle to obtain an easy-to-handle bound on the marginal log-likelihood, i.e., the evidence lower bound (ELBO). We use $p(z)=N(1,0)$ as the latent variable prior. ELBO is an easy-to-process lower bound on the log-likelihood, and thus maximizes the inference $p_{\theta}(x|z)$. $\mathbb E_{z \sim q_{\phi}(z|x)[log(p_{\phi}(x|z))]}$ can be interpreted as the reconstruction error, since maximizing it will make the output of the decoder similar to the input of the encoder, and $D_{KL}[q_{\phi}(z|x)||p(z)]$ is the KL scatter (see *** for details), a value that measures the similarity of two distributions and that ensures that the latent representation is Gaussian, such that data points with similar characteristics have similar Gaussian representations.

	We have outlined the general idea behind latent variable models, but we still need to specify the approximate (variational) posterior $q_{\phi}(z|x)$ and the model likelihood $log(p_{\theta}(x|z))$. A common choice for approximating the posterior is to decompose the Gaussian distribution
	\begin{equation}\label{prop}
		q_{\phi}(z|x) = \prod^d_{j=1} N(z_j|\mu_j,\sigma_j^2)
	\end{equation}
	where $\mu$ and $\sigma$ denote the mean and standard deviation of the model.
	
	In summary, VAE is an encoder network based on extracting the mean and variance of a potential Gaussian representation $q_{\phi}(z|x)$ of $p(z|x)$. The decoder in VAE, which uses a Gaussian $q_{\phi}(z|x)$ sample as input, generates new samples according to the distribution $p_{\theta}(x|z)$. We compute all parameters by maximizing ELBO using backpropagation, but the Gaussian distribution is random and non-integrable. Therefore a deterministic and differentiable mapping between the output of the encoder and the input of the decoder needs to be established. We need to represent the random variable $z$ as a differentiable and invertible variation $g$ of another auxiliary random variable $\epsilon$ (i.e., $z = g(\mu,\sigma,\epsilon)$). We employ a re-referencing trick such that $g(\mu,\sigma,\epsilon) = \mu + \sigma \odot \epsilon$ i.e.
	\begin{equation}\label{reparam}
		z  = \mu + \sigma \odot \epsilon
	\end{equation}
	where $\epsilon \sim N(0,1)$ and $\odot$ is the product of elements.
	
	\begin{figure}
		\centering
		\includegraphics[scale=0.25]{vae_style2}
		\caption{}
		\label{fig:vaestyle2}
	\end{figure}
	
	
	\begin{figure}[b]
		\includegraphics[scale=0.3]{vae}
		\caption{\label{fig:epsart} A figure caption. The figure captions are automatically numbered.}
		\label{fig:vae}
	\end{figure}
	
	\section{spin Variational graph auto-encoder}
	We want to construct a graph variational self-encoder that applies the idea of VAE to graph structure data. We want our graph variational self-encoder to generate new Ising configurations of different sizes in steady state. However, we cannot use VAE directly because traditional convolution can only be applied to Ising configurations of the same size, and different Ising configurations are equivalent to different graph structures with different numbers of nodes resulting in the inability to use convolution directly. At this point, we can use the graph convolution mentioned above, which can effectively describe different graph structures, train different sized configurations in the same neural network, and get the generation effect we want, i.e., generate steady-state Ising configurations of different sizes.	
	\subsection{Encoder}
	The first two GCN layers generate a high-dimensional feature matrix, while we consider the edge information in the graph structure, the edge features are involved in the convolutional aggregation, refer to the previous update formula of the graph convolution can get our new update formula.
	\begin{equation}\label{upgradeFormula}
		\mathbf{x}^{\prime}_i = \mathbf{W}_1 \mathbf{x}_i + \mathbf{W}_2\sum_{j \in \mathcal{N}(i)} e_{j,i} \cdot \mathbf{x}_j
	\end{equation}

	$e_{j,i}$ denotes the edge weights from source node $j$ to target node $i$.
	The result of the upper convolution in the convolution operation is used as the input to the lower convolution to perform the stacking of convolutions.The third convolutional layer we use two different convolutional networks to obtain $\mu,\log{\sigma^2}$ respectively
	\begin{equation}\label{mu}
		\mu = GCN_{\mu}(\hat X,A) \\
	\end{equation}	
	\begin{equation}\label{log}
		\log{\sigma^2} = GCN_{\log{\sigma^2}}(\hat X,A)
	\end{equation}
	Then we can sample Z by eqs.~\ref{reparam}. from the distribution.
	\begin{figure*}
		\includegraphics[scale=0.2]{spin_vgae2}
		\caption{\label{fig:wide}Use the figure* environment to get a wide figure that spans the page in \texttt{twocolumn} formatting.}
		\label{fig:spinvgae2}
	\end{figure*}
	\subsection{Decoder}
	Our sampling result z is a high-dimensional representation of the nodes. In the decoder part, since the connection between the nodes of the Ising model is fixed, we can ignore the edge properties of the Ising model in the decoder part and use a fully connected neural network for each node in the potential variable z to reduce its dimensionality to 1. At this point we can think of the reconstruction problem as classifying each node, Using the sigmod activation function, we then classify them by value. We obtain our generated reconstruction model
	\begin{equation}\label{x_i}
		\overline x_i = MLP(z_i)
	\end{equation}
	$z_i$ represents the high-dimensional representation of the nodes, which are transformed into the reconstructed nodes we need by the fully-connected neural network. Since we perform the fully-connected neural network for each high-dimensional node and finally downscale to 1 dimension, our model can adapt to various sizes of configurations. We realize that one model can generate multiple configurations of various sizes, Meanwhile, because of the mixed training of multiple models, some noise is added to the training, which enhances the robustness of the model and the model generation is better.
	
	After several iterations, we can consider the model training successful when the value of the loss function is less than the threshold, and then we can save the model and the trained $\mu$ and $\sigma$ and use the decoder to generate the configuration we want. Our model can generate multiple sizes of configurations, just let the sizes you need to train together, and after the training is finished, we can use the configurations under the size to fix the model to the size we need, then we can generate the configurations under the size we want.

	\section{SIMULATION RESULTS AND ANALYSIS}
	To verify the effectiveness of the network described above, we build an Ising simulator that produces Ising states in a 32 × 32 lattice near its critical temperature. The model we are simulating is descried by a Hamiltonian of the eqs.\ref{H}
	
		\begin{table*}
			\caption{\label{tab:table3}This is a wide table that spans the full page
				width in a two-column layout. It is formatted using the
				\texttt{table*} environment. It also demonstates the use of
				\textbackslash\texttt{multicolumn} in rows with entries that span
				more than one column.}
			\begin{ruledtabular}
				\begin{tabular}{ccccc}
					&\multicolumn{2}{c}{$D_{4h}^1$}&\multicolumn{2}{c}{$D_{4h}^5$}\\
					Ion&1st alternative&2nd alternative&lst alternative
					&2nd alternative\\ \hline
					K&$(2e)+(2f)$&$(4i)$ &$(2c)+(2d)$&$(4f)$ \\
					Mn&$(2g)$\footnote{The $z$ parameter of these positions is $z\sim\frac{1}{4}$.}
					&$(a)+(b)+(c)+(d)$&$(4e)$&$(2a)+(2b)$\\
					Cl&$(a)+(b)+(c)+(d)$&$(2g)$\footnotemark[1]
					&$(4e)^{\text{a}}$\\
					He&$(8r)^{\text{a}}$&$(4j)^{\text{a}}$&$(4g)^{\text{a}}$\\
					Ag& &$(4k)^{\text{a}}$& &$(4h)^{\text{a}}$\\
				\end{tabular}
			\end{ruledtabular}
		\end{table*}
		
		Insert an image using either the \texttt{graphics} or
		\texttt{graphix} packages, which define the \verb+\includegraphics{#1}+ command.
		(The two packages differ in respect of the optional arguments 
		used to specify the orientation, scaling, and translation of the image.) 
		To create an alignment, use the \texttt{tabular} environment. 
		
		The best place to locate the \texttt{figure} or \texttt{table} environment
		is immediately following its first reference in text; this sample document
		illustrates this practice for Fig.~\ref{fig:epsart}, which
		shows a figure that is small enough to fit in a single column. 
		
		In exceptional cases, you will need to move the float earlier in the document, as was done
		with Table~\ref{tab:table3}: \LaTeX's float placement algorithms need to know
		about a full-page-width float earlier. 
		
		Fig.~\ref{fig:wide}
		has content that is too wide for a single column,
		so the \texttt{figure*} environment has been used.%
		\begin{table}[b]
			\caption{\label{tab:table4}%
				Numbers in columns Three--Five are aligned with the ``d'' column specifier 
				(requires the \texttt{dcolumn} package). 
				Non-numeric entries (those entries without a ``.'') in a ``d'' column are aligned on the decimal point. 
				Use the ``D'' specifier for more complex layouts. }
			\begin{ruledtabular}
				\begin{tabular}{ccddd}
					One&Two&
					\multicolumn{1}{c}{\textrm{Three}}&
					\multicolumn{1}{c}{\textrm{Four}}&
					\multicolumn{1}{c}{\textrm{Five}}\\
					%\mbox{Three}&\mbox{Four}&\mbox{Five}\\
					\hline
					one&two&\mbox{three}&\mbox{four}&\mbox{five}\\
					He&2& 2.77234 & 45672. & 0.69 \\
					C\footnote{Some tables require footnotes.}
					&C\footnote{Some tables need more than one footnote.}
					& 12537.64 & 37.66345 & 86.37 \\
				\end{tabular}
			\end{ruledtabular}
		\end{table}
		
		The content of a table is typically a \texttt{tabular} environment, 
		giving rows of type in aligned columns. 
		Column entries separated by \verb+&+'s, and 
		each row ends with \textbackslash\textbackslash. 
		The required argument for the \texttt{tabular} environment
		specifies how data are aligned in the columns. 
		For instance, entries may be centered, left-justified, right-justified, aligned on a decimal
		point. 
		Extra column-spacing may be be specified as well, 
		although REV\TeX~4 sets this spacing so that the columns fill the width of the
		table. Horizontal rules are typeset using the \verb+\hline+
		command. The doubled (or Scotch) rules that appear at the top and
		bottom of a table can be achieved enclosing the \texttt{tabular}
		environment within a \texttt{ruledtabular} environment. Rows whose
		columns span multiple columns can be typeset using the
		\verb+\multicolumn{#1}{#2}{#3}+ command (for example, see the first
		row of Table~\ref{tab:table3}).%
		
		Tables~\ref{tab:table1}, \ref{tab:table3}, \ref{tab:table4}, and \ref{tab:table2}%
		\begin{table}[b]
			\caption{\label{tab:table2}
				A table with numerous columns that still fits into a single column. 
				Here, several entries share the same footnote. 
				Inspect the \LaTeX\ input for this table to see exactly how it is done.}
			\begin{ruledtabular}
				\begin{tabular}{cccccccc}
					&$r_c$ (\AA)&$r_0$ (\AA)&$\kappa r_0$&
					&$r_c$ (\AA) &$r_0$ (\AA)&$\kappa r_0$\\
					\hline
					Cu& 0.800 & 14.10 & 2.550 &Sn\footnotemark[1]
					& 0.680 & 1.870 & 3.700 \\
					Ag& 0.990 & 15.90 & 2.710 &Pb\footnotemark[2]
					& 0.450 & 1.930 & 3.760 \\
					Au& 1.150 & 15.90 & 2.710 &Ca\footnotemark[3]
					& 0.750 & 2.170 & 3.560 \\
					Mg& 0.490 & 17.60 & 3.200 &Sr\footnotemark[4]
					& 0.900 & 2.370 & 3.720 \\
					Zn& 0.300 & 15.20 & 2.970 &Li\footnotemark[2]
					& 0.380 & 1.730 & 2.830 \\
					Cd& 0.530 & 17.10 & 3.160 &Na\footnotemark[5]
					& 0.760 & 2.110 & 3.120 \\
					Hg& 0.550 & 17.80 & 3.220 &K\footnotemark[5]
					&  1.120 & 2.620 & 3.480 \\
					Al& 0.230 & 15.80 & 3.240 &Rb\footnotemark[3]
					& 1.330 & 2.800 & 3.590 \\
					Ga& 0.310 & 16.70 & 3.330 &Cs\footnotemark[4]
					& 1.420 & 3.030 & 3.740 \\
					In& 0.460 & 18.40 & 3.500 &Ba\footnotemark[5]
					& 0.960 & 2.460 & 3.780 \\
					Tl& 0.480 & 18.90 & 3.550 & & & & \\
				\end{tabular}
			\end{ruledtabular}
			\footnotetext[1]{Here's the first, from Ref.~\onlinecite{feyn54}.}
			\footnotetext[2]{Here's the second.}
			\footnotetext[3]{Here's the third.}
			\footnotetext[4]{Here's the fourth.}
			\footnotetext[5]{And etc.}
		\end{table}
		show various effects.
		A table that fits in a single column employs the \texttt{table}
		environment. 
		Table~\ref{tab:table3} is a wide table, set with the \texttt{table*} environment. 
		Long tables may need to break across pages. 
		The most straightforward way to accomplish this is to specify
		the \verb+[H]+ float placement on the \texttt{table} or
		\texttt{table*} environment. 
		However, the \LaTeXe\ package \texttt{longtable} allows headers and footers to be specified for each page of the table. 
		A simple example of the use of \texttt{longtable} can be found
		in the file \texttt{summary.tex} that is included with the REV\TeX~4
		distribution.
		
		There are two methods for setting footnotes within a table (these
		footnotes will be displayed directly below the table rather than at
		the bottom of the page or in the bibliography). The easiest
		and preferred method is just to use the \verb+\footnote{#1}+
		command. This will automatically enumerate the footnotes with
		lowercase roman letters. However, it is sometimes necessary to have
		multiple entries in the table share the same footnote. In this case,
		there is no choice but to manually create the footnotes using
		\verb+\footnotemark[#1]+ and \verb+\footnotetext[#1]{#2}+.
		\texttt{\#1} is a numeric value. Each time the same value for
		\texttt{\#1} is used, the same mark is produced in the table. The
		\verb+\footnotetext[#1]{#2}+ commands are placed after the \texttt{tabular}
		environment. Examine the \LaTeX\ source and output for
		Tables~\ref{tab:table1} and \ref{tab:table2}
		for examples.
		
		Video~\ref{vid:PRSTPER.4.010101} 
		illustrates several features new with REV\TeX4.2,
		starting with the \texttt{video} environment, which is in the same category with
		\texttt{figure} and \texttt{table}.%
	\begin{figure}[htbp]
	\centering
	\begin{subfigure}{0.4\linewidth}
		\centering
		\includegraphics[scale=0.1]{16mix32_32_T_3_E}
		\caption{chutian3}
		\label{chutian3}%文中引用该图片代号
	\end{subfigure}
	\centering
	\begin{subfigure}{0.4\linewidth}
		\centering
		\includegraphics[scale=0.1]{16mix32_32_T_3_M}
		\caption{chutian3}
		\label{chutian3}%文中引用该图片代号
	\end{subfigure}
	\centering
	\begin{subfigure}{0.4\linewidth}
		\centering
		\includegraphics[scale=0.1]{16mix32_16_T_3_E}
		\caption{chutian3}
		\label{chutian3}%文中引用该图片代号
	\end{subfigure}
	\begin{subfigure}{0.4\linewidth}
		\centering
		\includegraphics[scale=0.1]{16mix32_16_M}
		\caption{chutian3}
		\label{chutian3}%文中引用该图片代号
	\end{subfigure}

	\caption{mother of chutian}
	\label{da_chutian}
	\end{figure}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{16mix32_32_T_3_E_sactter}
		\caption{Comparison of energy scatter diagrams generated by the hybrid model of 16 and 32 sizes for 32 sizes at T=3 temperature with those generated by the MCMC method}
		\label{fig:16mix3232t3esactter}
	\end{figure}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{32_T_3_M_scatter}
		\caption{Comparison of Magnetization scatter diagrams generated by a separate 32-size model at T=3 temperature with those generated by conventional methods}
		\label{fig:32t3mscatter}
	\end{figure}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{16mix32_32_T_3_M_scatter}
		\caption{Comparison of Magnetization scatter diagrams generated by the hybrid model of 16 and 32 sizes for 32 sizes at T=3 temperature with those generated by the MCMC method}
		\label{fig:16mix3232t3mscatter}
	\end{figure}
	
		The \verb+\setfloatlink+ command causes the title of the video to be a hyperlink to the
		indicated URL; it may be used with any environment that takes the \verb+\caption+
		command.
		The \verb+\href+ command has the same significance as it does in the context of
		the \texttt{hyperref} package: the second argument is a piece of text to be 
		typeset in your document; the first is its hyperlink, a URL.
		
		\textit{Physical Review} style requires that the initial citation of
		figures or tables be in numerical order in text, so don't cite
		Fig.~\ref{fig:wide} until Fig.~\ref{fig:epsart} has been cited.
		
		\begin{acknowledgments}
			We wish to acknowledge the support of the author community in using
			REV\TeX{}, offering suggestions and encouragement, testing new versions,
			\dots.
		\end{acknowledgments}
		
		\appendix
		
		\section{Appendixes}
		
		To start the appendixes, use the \verb+\appendix+ command.
		This signals that all following section commands refer to appendixes
		instead of regular sections. Therefore, the \verb+\appendix+ command
		should be used only once---to setup the section commands to act as
		appendixes. Thereafter normal section commands are used. The heading
		for a section can be left empty. For example,
		\begin{verbatim}
			\appendix
			\section{}
		\end{verbatim}
		will produce an appendix heading that says ``APPENDIX A'' and
		\begin{verbatim}
			\appendix
			\section{Background}
		\end{verbatim}
		will produce an appendix heading that says ``APPENDIX A: BACKGROUND''
		(note that the colon is set automatically).
		
		If there is only one appendix, then the letter ``A'' should not
		appear. This is suppressed by using the star version of the appendix
		command (\verb+\appendix*+ in the place of \verb+\appendix+).
		
		\section{A little more on appendixes}
		
		Observe that this appendix was started by using
		\begin{verbatim}
			\section{A little more on appendixes}
		\end{verbatim}
		
		Note the equation number in an appendix:
		\begin{equation}
			E=mc^2.
		\end{equation}
		
		\subsection{\label{app:subsec}A subsection in an appendix}
		
		You can use a subsection or subsubsection in an appendix. Note the
		numbering: we are now in Appendix~\ref{app:subsec}.
		
		Note the equation numbers in this appendix, produced with the
		subequations environment:
		\begin{subequations}
			\begin{eqnarray}
				E&=&mc, \label{appa}
				\\
				E&=&mc^2, \label{appb}
				\\
				E&\agt& mc^3. \label{appc}
			\end{eqnarray}
		\end{subequations}
		They turn out to be Eqs.~(\ref{appa}), (\ref{appb}), and (\ref{appc}).
		
		% The \nocite command causes all entries in a bibliography to be printed out
		% whether or not they are actually referenced in the text. This is appropriate
		% for the sample file to show the different styles of references, but authors
		% most likely will not want to use it.
		\nocite{*}
		\bibliographystyle{plain}
		\bibliography{apssamp}% Produces the bibliography via BibTeX.
		
	\end{document}