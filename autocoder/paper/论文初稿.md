<h1 align='center'>前言</h1>

由于深度学习有从复杂系统中提取基本特征的强大能力，它在图像识别，语音处理，生物系统等领域都具有广泛的应用。特别是在凝聚态物理领域，深度学习展现了可以精确逼近复杂物理系统的独特能力。在过去的几年，传统的神经网络在凝聚态物理领域无论是在提取特征还是在生成模型方面都取得了不错的成果。然而由于构型尺寸的限制，导致神经网络只能生成单一尺寸的构型，限制了神经网络的泛化性而且传统的深度学习算法假设数据样本之间彼此独立，但是在实际的物理构型中，每个自旋节点都会与构型中的其他节点以及边相关。图神经网络的出现解决了这一问题，图是不规则的，它可以适应任意尺寸的构型，并且可以捕获不同节点之间的相互依赖关系。变分自编码是一种无监督学习算法，可以有效地模拟训练数据的分布。在本文中，我们使用图神经网络这一特性与变分自编码相结合，构建Ising模型模拟器，生成的Ising模型与传统蒙特卡罗方法模拟的数据非常相似。结果表明，图神经网络在凝聚态物理领域是一种有潜力的计算工具。

<h1 align='center'>引言</h1>

今年来，物理学家已经开始采用深度学习技术来研究凝聚态物理领域。大多数任务都是通过监督学习算法来完成的。在监督学习中，算法是对带标签的数据进行训练，将标签分配给数据点。经过成功的训练，可以对以前未见过的数据的标签进行高精度的预测。除了有监督学习以外，还有一些无监督学习算法可以在无标签数据中找到结构。已经有可能使用无监督的学习技术来重现Ising模型的蒙特卡罗采样状态，以无监督的方式发现相变已经成为主流。受限制玻尔兹曼机，变分自编码和生成对抗网络等都在Ising模型的模拟上广泛应用，并取得了不错的效果。但是这些模型在训练时都无法兼顾物理模型分子与分子之间的相互作用和分子之间边的影响。同时很多物理模型的微观结构时不规则的，生成模型的泛用性差，一般只能适应一种温度下的一种尺寸。这就需要一种能考虑非结构化数据，并且能考虑节点与节点，节点与边的关系的深度神经网络。

幸运的是，近年来深度学习领域关于图神经网络的研究热情日益高涨，图神经网络处理非结构化数据的出色能力使得其在物理，生物和化学等方面的应用尤为突出。大多数物理模型本质上就是一种图结构。图卷积网络可以聚合邻居节点的信息，相比较传统卷积网络，图卷积网络更善于编码图的结构信息，能够学习到更好更深层次的表示特征。图变分自编码是一种无监督图生成算法，是图神经网络与生成网络相结合的产物，它能够有效利用数据的图结构去模拟训练数据的分布。

在这篇文章中，我们将图卷积网络整合到图变分自编码器框架中构建Ising模型模拟器，用来模拟不同温度下的Ising模型构型。经过训练的模拟器能够有效的生成Ising模型状态，其物理特性与传统蒙特卡罗模拟得到的结果并无不同。并且可以适应多种不同的尺寸并且极大的缩短了模拟时间。我们将首先介绍一般形式的ISing模型，图变分神经网络与图卷积网络。在文章的核心部分，我们构建一个与图卷积网络相结合的图变分自编码器，模拟不同温度下的2D Ising模型，并通过将生成的数据与来自蒙特卡罗模拟的数据进行比较来验证我们方法的有效性模拟。

### Ising模型与图神经网络

#### Ising模型

在统计物理中，Ising模型被描述为某些晶格中具有耦合相互作用的二元自旋的集合。考虑N个自旋s={$s_i$}可以取值$\pm$ 1,其中索引i标记自旋$s_i$的位置,Ising系统的标准哈密顿量仅包括最近邻相互作用,并且每个自旋方向可能是"向上"(+1)或"向下"(-1),尽管广义模型可能包括长程相互作用和更多的自旋选择方向.标准哈密顿量是:

$$
H = -J\sum_{neighbors}S_i*S_j
$$

#### 图神经网络

图是一种数据结构，它对一组对象（节点）及其关系（边）进行建模。今年来，由于图结构的强大表现力，用机器学习方法分析图的研究越来越受到重视。图神经网络是指使用神经网络学习图结构数据，提取和发掘图结构数据中的特征和模式，满足聚类，分类，预测，分割，生成等图学习任务需求的算法总称。图神经网络发展的一个动机源于卷积神经网络。卷积神经网络的广泛应用带来了机器学习领域的突破并开启了深度学习时代。然而卷积神经网络只能在规则的欧式空间数据中提取潜在特征，不能很好的对应现实中复杂多变的图数据，如何将卷积神经网络应用于图结构这一非欧式空间成为图神经网络模型重点解决的问题。以下给出图神经网络中相关符号的说明和定义

**定义1.** 图是由节点和连接节点的边所构成的，通常记为$G=(V,E)$。其中$V=\{v_1,v_2,...,v_n\}$代表节点集合,$E=\{e_1,e_2,...,e_n\}$代表边集合,边也可以用$(v_1,v_2)$的方式来表示.通常节点也被称为顶点或者交点.边也被成为链接或者弧.通用的图表示是一个五元组:$G(V,E,A,X,D)$.其中A代表图的邻接矩阵,X代表节点的特征矩阵,D代表度矩阵.

**定义2.** 邻接矩阵:图的临界矩阵指用于表示图中节点的连接情况的矩阵.该矩阵可以是二值的,也可以是带权的.对于有N个节点的无向图来说,邻接矩阵是一个N*N的实对称矩阵.

**定义3.** 度矩阵:节点的度表示与该节点相连的边的数量.图的度矩阵即用于描述图中每个节点的度的矩阵.度矩阵是一个对角矩阵,对于无向图来说,一般只使用入度矩阵或者出度矩阵.

**定义4.** 组合拉普拉斯矩阵,又称标准拉普拉斯矩阵,由对角矩阵和邻接矩阵组合而成:

$$
L = D -A
$$

该矩阵只在中心节点和一阶相连的节点上有非零元素,其余之处均为零.拉普拉斯矩阵也是图的一种表现形式.
**定义5.** 归一化拉普拉斯矩阵

$$
L^{sym} = I - D^{-1/2}AD^{-1/2}
$$

其元素值为:

$$
L^{sym}_{(i,j)} = 
\begin{cases}
1,& i=j且deg(v_i)\ne0\\
\frac {-1}{\sqrt{deg(v_i)deg(v_j)}},&{i}\ne{j}并且v_i与v_j相连\\
0,& \text{otherwise}
\end{cases}
$$

其中$deg(v)$表示为节点$v$的度

#### 图卷积网络

图卷积网络是由传统的卷积神经网络引申出的图卷积网络，图卷积可以两种，基于频谱的方法和基于空间的方法。基于频谱的方法从图信号处理的角度，引入滤波器来定义图卷积，因此基于频谱的图卷积可以理解为从图信号中去除噪音。基于空间的图卷积方法通过汇集邻居节点的信息来构建图卷积。

在了解图卷积之前，我们先要对卷积操作有一个基本的概念。卷积网络最开始应用在图像上，我们将图像上的像素点作为特征来进行卷积操作提取特征。用随机的共享的卷积和得到像素点的加权和从而提取到某种特定的特征，然后用反向传播来优化卷积核参数就可以自动的提取特征，是卷积神经网络提取特征的基石。可以这么认为，~~卷积操作将一张图像中的特征信息聚合起来，提取出更有用的特征。~~ 卷积操作相当于将周围的像素点信息聚合到卷积核中央像素点上，提取出该部分的特征。

所以我们在图领域想利用图像领域的卷积，使用一个通用的范式来进行图特征的提取，与图像类似，我们在想得到节点的特征表示时，可以通过聚合邻居节点的信息，可以这样理解，图中的每个节点无时无刻不因为邻居节点核更远的点影响而在改变自己的状态直到最终的平衡，关系越亲近的邻居影响越大。

##### 图卷积通式

对于图卷积网络来说，我们想要学习到它对于每个节点的特征表示，任意的图卷积都可以写成这样的一个非线性函数：

$$
H^{l+1} = f(H^l,A)
$$

$H^0 = X$ 为第一层的输入,$l$代表神经网络的层数,A代表邻接矩阵.

传统的图卷积网络做法就是将邻居节点的信息加入到本节点当中:

$$
f(H^l,A) = \sigma(A H^l W^l)
$$

$W^l$代表第l层的参数矩阵,$\sigma$代表激活函数. 根据矩阵乘法可以看出上式中的每个节点都结合了相邻节点的信息,但由于仅仅使用了相邻矩阵,相邻矩阵对角线为0,无法体现节点自身的信息,所以我们使用拉普拉斯矩阵来代替邻接矩阵

$$
f(H^l,A) = \sigma(L H^l W^l)
$$

上式中引入了拉普拉斯矩阵,从而解决了没有考虑自身节点信息自传递的问题,但是由于没有被规范化,我们将拉普拉斯矩阵规则化得到图卷积

$$
f(H^l,A) = \sigma(L^{sym} H^l W^l)
$$

上面讲述的都是以矩阵形式计算，我们也可以从单个节点角度来观察公式，对于第$l+1$层的节点特征$h^{l+1}_i$,对于他的邻接结点$j \in N$,$N$是结点$i$的所用邻居结点的集合,所以图神经网络的更新公式同样可以描写为

$$
h^{l+1}_i = f(h^{l}_i) = \sigma(h^{l}_i \bullet W_1^{l+1} + \sum_j h^{l}_j \bullet W^{l+1}_2)
$$

这也是图卷积网络的通式写法,除了一些特殊的图神经网络,几乎所有的图卷积都可以以这种形式表达.

##### Weisfeiler-Lehman算法

WL算法是一种区分同构图的算法，它的方法与现在的GNN有着异曲同工之妙，所以在图神经网络领域也是一个比较重要的算法。什么是同构图？

定义：假设$G=(V,E)$和$G1=(V1,E1)$是两个图,如果存在一个双射$m:V \to V1$,使得对所有的$x,y \in V$均有$(x,y) \in E$等价于$m(x)m(y) \in E1$,则称G和G1是同构的.

**1-维WL(1-wl)算法**.该算法为每个节点赋予一种标签,并不断聚合邻居节点的信息,修改节点的标签,直到标签不再变化.

输入:图$G1=(V1,E1,X1)$和图$G2=(V2,E2,X2)$

1. $c^{(0)}_{v} \leftarrow HASH(X1_{v})(\forall v \in V1)$
2. $d^{(0)}_{u} \leftarrow HASH(X2_{u})(\forall u \in V2)$
3. for l = 1,2,...(直到收敛)
   * $if\{\{c^{(l-1)}_v | v \in V1\}\} \ne \{\{d^{(l-1)}_u | u \in V2\}\}$ then return "non-isomorphis"
   * $c^{(l)} \leftarrow HASH(c_v^{(l-1)},\{\{c^{(l-1)} | w \in N_{G1}(v)\}\}(\forall v \in V1)$
   * $d^{(l)} \leftarrow HASH(d_u^{(l-1)},\{\{d^{(l-1)} | w \in N_{G2}(u)\}\}(\forall u \in V2)$
4. 返回可能同构

HASH函数是一种单射函数.如果1-WL算法输出"非同构",那么G1与G2就不是同构的图,但即使1-WL算法输出"可能同构",G1与G2仍有可能非同构.例如,对于**图?** 中的图,1-WL算法输出"可能同构",然而他们并非同构.

**K-维WL算法**.1-维WL的推广,K-维WL为每k个节点组成的元组赋予一种标签

1. $c^{(0)}_{v} \leftarrow HASH(G1[v])(\forall v \in V1^k)$
2. $d^{(0)}_{u} \leftarrow HASH(G2[u])(\forall u \in V2^k)$
3. for l = 1,2,...(直到收敛)
   
   * $if\{\{c^{(l-1)}_v | v \in V1^k\}\} \ne \{\{d^{(l-1)}_u | u \in V2^k\}\}$ then return "non-isomorphis"
   * $c^{(l)}_{v,i} \leftarrow \{\{c^{(l-1)}_w | w \in N^{k-WL}_{G1,i}(v)\}\}(\forall v \in V1^k,i \in [k])$
   * $c_v^{(l)} \leftarrow HASH(c_v^{(l-1)},c_{v,1}^{(l)},c_{v,2}^{(l)},...,c_{v,k}^{(l)})(\forall v \in V1)$
   * $d^{(l)}_{u,i} \leftarrow \{\{d^{(l-1)}_w | w \in N^{k-WL}_{G2,i}(u)\}\}(\forall u \in V2^k,i \in [k])$
   * $d_u^{(l)} \leftarrow HASH(d_v^{(l-1)},d_{v,1}^{(l)},d_{v,2}^{(l)},...,d_{v,k}^{(l)})(\forall u \in V2)$
4. 返回可能重构

其中,$N^{k-WL}_{G1,i}((v_1,v_2,...,v_k)) = \{(v_1,...,v_{i-1},w,v_{i+1},...,v_k) | w \in V1\}$ 是第i个邻居节点,它使用G1中的每个节点替换k元组中的第i个元素.HASH是一种单射函数,它为相同的同构型赋予相同的标签.

##### 高阶图卷积

高阶图卷积网络的创建是受到了Weisfeiler-Lehman算法(以下简称WL算法)灵感的启发,所谓高阶图卷积从最通俗的话来讲就是将几个节点看作是一个节点,1-GNN就是一个节点就是一个节点,2-GNN就是两个节点看作一个节点,以此类推,来提出不同情况下的结果.

在高阶图卷积这篇文章中,作者指出,在区分非同构图方面,WL算法是GNN的上限,当GNN有合适的参数时,GNN区分同构图的能力能够与WL算法达到相同.GNN相对于WL算法,在检测连续的结点属性时,会有更加灵活的能力.在Ising构型的蒙特卡罗模拟中,我们有wollf算法作为集团演化方法的一种,wollf算法就是将连续的相同属性结点认为是一种团簇,构型中存在着多个团簇同时进行翻转,这与高阶图卷积的思想不谋而合.作为高阶图卷积,我们将图中k个有关联的结点认为是同一个结点,并于其同样k阶的结点作为邻居有相互作用,这样我们通过高阶图卷积可以学习到Ising构型中更高维的特征,从而使我们的下游任务更加准确.

同时文章指出(k+1)WL算法能检测到k-WL算法不能检测到的一些同构图.所以维度越高,我们提取的特征就越精准.所以作者借鉴1-WL算法到K-WL算法的拓展,将图神经网络拓展到高阶图神经网络.具体的更新公式为:

$$
f^{(t)}_{k,L}(s) = \sigma(f^{(t-1)}_{k,L}(s) \bullet W_1^{(t)} + \sum_{u \in N_L(s)}f^{(t)}_{k,L}(u) \bullet W^{(t)}_2)
$$

其中s表示由k个结点组成的subgraph子图,u是这个子图的邻居子图,其中邻居子图的定义是:

$$
N(s) = \{ t \in V^k | |s \cap t| = k - 1 \}
$$

邻居子图的含义为k个结点组成的子图,其邻居子图必须和其有且仅有k-1个公共节点.其邻居子图的含义与K-WL算法对于邻居的含义是相同的.

#### 变分自编码器

变分自编码器是一种生成模型,由编码器和解码器两部分网络组成,它是包含隐变量的一种模型.在变分自编码器当中,我们基于这样的假设:
我们的样本x是某些隐变量z(latent variable)通过某种映射产生出的,而z也不是一个固定值,而是服从一个分布:$z \sim P_{\theta}(z)$,则$x \sim p_{\theta}(x|z)$,这里的P是由参数$\theta$决定的分布族,而$\theta$就是我们需要找到的真实分布.所以我们最根本的目标就是确定$z$和$\theta$的值,这样就能够得到样本x.为了推断$p_{\theta}(x|z)$,我们需要最大化边际对数似然$log(p_{\theta}(x))$我们可以重写该式:

$$
\begin{aligned}
log(p_{\theta}(x)) &= log \int p_{\theta}(x|z)dz = log \in p_{\theta}(x|z)p(z)dz \\
&= log \int p_{\theta}(x|z)p(z)\frac{q_{\phi}(z|x)}{q{\phi}(z|x)}dz\\
&\ge \mathbb E_{z \sim q_{\phi}(z|x)}[log(p_{\theta}(x|z)] - D_{KL}[q_{\phi}(z|x)||p(z)]\\
&:= \varepsilon(x,\theta,\phi)
\end{aligned}
$$

我们在第四步应用了Jensen不等式.在第二步中对潜在变量z的积分通常是棘手的,因此我们引入了带有参数集$\phi$的近似后验分布$q_{\phi}(z|x)$并使用变分推理原理来获得边缘对数似然的易处理界限,即证据下界(ELBO).我们使用$p(z)=N(1,0)$作为潜在变量先验.ELBO是对数似然的易处理下界,因此可以最大化推断$p_{\theta}(x|z)$.$\mathbb E_{z \sim q_{\phi}(z|x)[log(p_{\phi}(x|z))]}$可以理解为重构误差,因为最大化它会使解码器的输出类似于编码器的输入,$D_{KL}[q_{\phi}(z|x)||p(z)]$是KL散度,一种衡量两个分布相似性的数值,它可以确保潜在表示是高斯的,使得具有相似特征的数据点具有相似的高斯表示.

#### 自旋图变分自编码

### 模拟结果与分析

### 总结

